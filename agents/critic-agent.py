"""
Critic Agent implementation.

This agent is responsible for evaluating the quality, correctness, and alignment
of the outputs generated by other agents, providing constructive feedback for
improvement.
"""

import logging
import json
from typing import Dict, List, Any, Optional

from agents.base_agent import BaseAgent
from utils.prompt_templates import CRITIC_SYSTEM_MESSAGE, CRITIC_EVALUATION_TEMPLATE

logger = logging.getLogger(__name__)

class CriticAgent(BaseAgent):
    """
    Critic Agent responsible for evaluating and providing feedback on outputs.
    
    The Critic assesses the quality, correctness, and alignment of outputs
    generated by other agents, providing structured feedback for iterative
    refinement and improvement. It can check for factual accuracy, logical
    consistency, and alignment with the original goals.
    """
    
    def __init__(self, **kwargs):
        """Initialize the Critic Agent with evaluation capabilities."""
        super().__init__(name="Critic Agent", **kwargs)
        
        # Evaluation metrics and thresholds
        self.evaluation_criteria = {
            "factual_accuracy": {"weight": 0.3, "threshold": 0.7},
            "logical_consistency": {"weight": 0.2, "threshold": 0.8},
            "completeness": {"weight": 0.2, "threshold": 0.7},
            "relevance": {"weight": 0.15, "threshold": 0.8},
            "clarity": {"weight": 0.15, "threshold": 0.7}
        }
        
        # Feedback history
        self.feedback_history = []
        
        logger.info(f"Initialized Critic Agent (ID: {self.agent_id})")
    
    async def process(self, evaluation_request: Dict) -> Dict:
        """
        Evaluate content based on various criteria and provide structured feedback.
        
        Args:
            evaluation_request: Dictionary containing what needs to be evaluated
                - content: The content to evaluate
                - original_request: The original user request or goal
                - content_type: Type of content (plan, execution_result, etc.)
                - context: Additional context for evaluation
                
        Returns:
            A dictionary containing the evaluation results:
                - scores: Scores for different criteria
                - overall_score: Weighted average score
                - feedback: Textual feedback with suggestions
                - action_needed: Suggested next action (accept, refine, reject)
        """
        content = evaluation_request.get("content", "")
        original_request = evaluation_request.get("original_request", "")
        content_type = evaluation_request.get("content_type", "general")
        context = evaluation_request.get("context", {})
        
        logger.info(f"Evaluating {content_type} content: {content[:50]}...")
        
        # Select the appropriate evaluation approach based on content type
        if content_type == "plan":
            evaluation_result = await self._evaluate_plan(content, original_request, context)
        elif content_type == "execution_result":
            evaluation_result = await self._evaluate_execution_result(content, original_request, context)
        elif content_type == "final_answer":
            evaluation_result = await self._evaluate_final_answer(content, original_request, context)
        else:
            evaluation_result = await self._evaluate_general_content(content, original_request, context)
        
        # Add to feedback history
        self.feedback_history.append({
            "content_type": content_type,
            "original_request": original_request,
            "evaluation": evaluation_result
        })
        
        # Store in memory if available
        if self.memory:
            await self.memory.store_evaluation(evaluation_result)
        
        return evaluation_result
    
    async def _evaluate_general_content(self, content: str, original_request: str, context: Dict) -> Dict:
        """
        Evaluate general content using the standard criteria.
        
        Args:
            content: The content to evaluate
            original_request: The original request or goal
            context: Additional context
            
        Returns:
            Evaluation results
        """
        # Generate prompt for evaluation
        prompt = CRITIC_EVALUATION_TEMPLATE.format(
            content=content,
            original_request=original_request,
            context=self._format_context(context),
            criteria=self._format_criteria()
        )
        
        # Generate evaluation
        evaluation_response = await self.generate_response(
            prompt=prompt,
            system_message=CRITIC_SYSTEM_MESSAGE
        )
        
        # Parse the evaluation response
        try:
            parsed_evaluation = self._parse_evaluation(evaluation_response)
        except Exception as e:
            logger.error(f"Error parsing evaluation: {str(e)}")
            # Fallback to a simpler evaluation approach
            parsed_evaluation = await self._generate_simple_evaluation(content, original_request)
        
        return parsed_evaluation
    
    async def _evaluate_plan(self, plan: str, original_request: str, context: Dict) -> Dict:
        """
        Evaluate a plan for completeness, feasibility, and alignment with goals.
        
        Args:
            plan: The plan to evaluate
            original_request: The original request or goal
            context: Additional context
            
        Returns:
            Evaluation results focused on plan quality
        """
        # Customize the evaluation criteria for plans
        plan_criteria = {
            "completeness": {"weight": 0.25, "threshold": 0.7},
            "feasibility": {"weight": 0.25, "threshold": 0.7},
            "alignment_with_goals": {"weight": 0.3, "threshold": 0.8},
            "logical_structure": {"weight": 0.2, "threshold": 0.7}
        }
        
        # Generate prompt for plan evaluation
        prompt = f"""
        Please evaluate this plan for addressing the original request.
        
        ORIGINAL REQUEST:
        {original_request}
        
        PLAN:
        {plan}
        
        CONTEXT:
        {self._format_context(context)}
        
        Evaluate the plan using these criteria:
        {self._format_criteria(plan_criteria)}
        
        For each criterion, provide:
        1. A score from 0.0 to 1.0
        2. Justification for the score
        3. Specific suggestions for improvement
        
        Also determine if the plan should be:
        - ACCEPTED (if overall score >= 0.8)
        - REFINED (if overall score >= 0.5 but < 0.8)
        - REJECTED (if overall score < 0.5)
        
        Format your response as a JSON object with these fields:
        - scores: Object with criterion name and score
        - justifications: Object with criterion name and justification
        - suggestions: Array of specific suggestions
        - overall_score: Weighted average score
        - action_needed: "accept", "refine", or "reject"
        - feedback: General feedback paragraph
        """
        
        # Generate evaluation
        evaluation_response = await self.generate_response(
            prompt=prompt,
            system_message=CRITIC_SYSTEM_MESSAGE
        )
        
        try:
            parsed_evaluation = self._parse_evaluation(evaluation_response, criteria=plan_criteria)
            
            # Add plan-specific checks
            if parsed_evaluation["overall_score"] >= 0.8:
                parsed_evaluation["action_needed"] = "accept"
            elif parsed_evaluation["overall_score"] >= 0.5:
                parsed_evaluation["action_needed"] = "refine"
            else:
                parsed_evaluation["action_needed"] = "reject"
                
            return parsed_evaluation
            
        except Exception as e:
            logger.error(f"Error parsing plan evaluation: {str(e)}")
            # Fallback
            return await self._generate_simple_evaluation(plan, original_request, content_type="plan")
    
    async def _evaluate_execution_result(self, result: str, original_request: str, context: Dict) -> Dict:
        """
        Evaluate execution results for accuracy, completeness, and relevance.
        
        Args:
            result: The execution result to evaluate
            original_request: The original request or goal
            context: Additional context including execution details
            
        Returns:
            Evaluation results focused on execution quality
        """
        # Customize criteria for execution results
        execution_criteria = {
            "factual_accuracy": {"weight": 0.3, "threshold": 0.7},
            "completeness": {"weight": 0.25, "threshold": 0.7},
            "relevance": {"weight": 0.25, "threshold": 0.8},
            "tool_usage_appropriateness": {"weight": 0.2, "threshold": 0.7}
        }
        
        # Generate prompt for execution evaluation
        step_info = context.get("step_info", "No step information provided")
        tools_used = context.get("tools_used", [])
        
        prompt = f"""
        Please evaluate this execution result for addressing the original request.
        
        ORIGINAL REQUEST:
        {original_request}
        
        STEP BEING EXECUTED:
        {step_info}
        
        TOOLS USED:
        {json.dumps(tools_used, indent=2) if tools_used else "No tools used"}
        
        EXECUTION RESULT:
        {result}
        
        ADDITIONAL CONTEXT:
        {self._format_context(context)}
        
        Evaluate the execution result using these criteria:
        {self._format_criteria(execution_criteria)}
        
        For each criterion, provide:
        1. A score from 0.0 to 1.0
        2. Justification for the score
        3. Specific suggestions for improvement
        
        Also determine if the execution should be:
        - ACCEPTED (if overall score >= 0.7)
        - RETRIED (if overall score >= 0.4 but < 0.7)
        - FAILED (if overall score < 0.4)
        
        Format your response as a JSON object with these fields:
        - scores: Object with criterion name and score
        - justifications: Object with criterion name and justification
        - suggestions: Array of specific suggestions
        - overall_score: Weighted average score
        - action_needed: "accept", "retry", or "fail"
        - feedback: General feedback paragraph
        """
        
        # Generate evaluation
        evaluation_response = await self.generate_response(
            prompt=prompt,
            system_message=CRITIC_SYSTEM_MESSAGE
        )
        
        try:
            parsed_evaluation = self._parse_evaluation(evaluation_response, criteria=execution_criteria)
            
            # Adjust action needed based on score thresholds for execution
            if parsed_evaluation["overall_score"] >= 0.7:
                parsed_evaluation["action_needed"] = "accept"
            elif parsed_evaluation["overall_score"] >= 0.4:
                parsed_evaluation["action_needed"] = "retry"
            else:
                parsed_evaluation["action_needed"] = "fail"
                
            return parsed_evaluation
            
        except Exception as e:
            logger.error(f"Error parsing execution evaluation: {str(e)}")
            # Fallback
            return await self._generate_simple_evaluation(result, original_request, content_type="execution_result")
    
    async def _evaluate_final_answer(self, answer: str, original_request: str, context: Dict) -> Dict:
        """
        Evaluate a final answer for accuracy, completeness, and clarity.
        
        Args:
            answer: The final answer to evaluate
            original_request: The original request or goal
            context: Additional context
            
        Returns:
            Evaluation results focused on answer quality
        """
        # Customize criteria for final answers
        answer_criteria = {
            "factual_accuracy": {"weight": 0.3, "threshold": 0.8},
            "completeness": {"weight": 0.25, "threshold": 0.8},
            "relevance": {"weight": 0.2, "threshold": 0.8},
            "clarity": {"weight": 0.15, "threshold": 0.7},
            "conciseness": {"weight": 0.1, "threshold": 0.7}
        }
        
        # Generate prompt for answer evaluation
        prompt = f"""
        Please evaluate this final answer for addressing the original request.
        
        ORIGINAL REQUEST:
        {original_request}
        
        FINAL ANSWER:
        {answer}
        
        CONTEXT:
        {self._format_context(context)}
        
        Evaluate the answer using these criteria:
        {self._format_criteria(answer_criteria)}
        
        For each criterion, provide:
        1. A score from 0.0 to 1.0
        2. Justification for the score
        3. Specific suggestions for improvement
        
        Also determine if the answer should be:
        - ACCEPTED (if overall score >= 0.8)
        - REFINED (if overall score >= 0.6 but < 0.8)
        - REJECTED (if overall score < 0.6)
        
        Format your response as a JSON object with these fields:
        - scores: Object with criterion name and score
        - justifications: Object with criterion name and justification
        - suggestions: Array of specific suggestions
        - overall_score: Weighted average score
        - action_needed: "accept", "refine", or "reject"
        - feedback: General feedback paragraph
        """
        
        # Generate evaluation
        evaluation_response = await self.generate_response(
            prompt=prompt,
            system_message=CRITIC_SYSTEM_MESSAGE
        )
        
        try:
            parsed_evaluation = self._parse_evaluation(evaluation_response, criteria=answer_criteria)
            
            # Set action needed
            if parsed_evaluation["overall_score"] >= 0.8:
                parsed_evaluation["action_needed"] = "accept"
            elif parsed_evaluation["overall_score"] >= 0.6:
                parsed_evaluation["action_needed"] = "refine"
            else:
                parsed_evaluation["action_needed"] = "reject"
                
            return parsed_evaluation
            
        except Exception as e:
            logger.error(f"Error parsing answer evaluation: {str(e)}")
            # Fallback
            return await self._generate_simple_evaluation(answer, original_request, content_type="final_answer")
    
    def _format_criteria(self, criteria: Optional[Dict] = None) -> str:
        """Format evaluation criteria into a string for prompting."""
        criteria = criteria or self.evaluation_criteria
        
        formatted = ""
        for name, props in criteria.items():
            formatted += f"- {name.replace('_', ' ').title()} (weight: {props['weight']}): "
            formatted += f"Minimum acceptable score is {props['threshold']}\n"
        
        return formatted
    
    def _format_context(self, context: Dict) -> str:
        """Format context dictionary into a string for prompting."""
        context_str = ""
        for key, value in context.items():
            if key not in ["step_info", "tools_used"]:  # These are handled separately
                context_str += f"{key}: {value}\n"
        return context_str
    
    def _parse_evaluation(self, evaluation_text: str, criteria: Optional[Dict] = None) -> Dict:
        """
        Parse the evaluation response from the LLM into a structured format.
        
        The function tries to extract JSON, then falls back to regex extraction
        if JSON parsing fails.
        
        Args:
            evaluation_text: The raw evaluation text from the LLM
            criteria: Optional criteria to use for weighted scoring
            
        Returns:
            A structured evaluation dictionary
        """
        import re
        import json
        
        # First, try to extract JSON from the response
        json_pattern = r'```json\s*(.*?)\s*```|(\{.*\})'
        json_match = re.search(json_pattern, evaluation_text, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(1) or json_match.group(2)
            try:
                evaluation = json.loads(json_str)
                
                # Ensure required fields are present
                required_fields = ["scores", "overall_score", "feedback"]
                if all(field in evaluation for field in required_fields):
                    # If action_needed is missing, derive it from the overall score
                    if "action_needed" not in evaluation:
                        criteria = criteria or self.evaluation_criteria
                        threshold = sum(c["threshold"] * c["weight"] for c in criteria.values())
                        evaluation["action_needed"] = "accept" if evaluation["overall_score"] >= threshold else "refine"
                    
                    return evaluation
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from evaluation response, falling back to regex")
        
        # If JSON extraction fails, try to extract scores using regex
        scores = {}
        criteria = criteria or self.evaluation_criteria
        
        # Extract scores for each criterion
        for criterion in criteria:
            score_pattern = rf'{criterion}.*?(\d+\.\d+|\d+)'
            score_match = re.search(score_pattern, evaluation_text, re.IGNORECASE)
            if score_match:
                try:
                    scores[criterion] = float(score_match.group(1))
                except ValueError:
                    scores[criterion] = 0.5  # Default if parsing fails
            else:
                scores[criterion] = 0.5  # Default if not found
        
        # Calculate overall score
        if scores:
            overall_score = sum(scores.get(criterion, 0) * props["weight"] for criterion, props in criteria.items())
        else:
            overall_score = 0.5  # Default if no scores found
        
        # Extract suggestions
        suggestions_pattern = r'suggestions:.*?((?:[-•*]\s*.*?(?:\n|$))+)'
        suggestions_match = re.search(suggestions_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)
        
        suggestions = []
        if suggestions_match:
            suggestion_items = re.findall(r'[-•*]\s*(.*?)(?:\n|$)', suggestions_match.group(1))
            suggestions = [item.strip() for item in suggestion_items if item.strip()]
        
        # Determine action needed based on overall score
        threshold = sum(c["threshold"] * c["weight"] for c in criteria.values())
        action_needed = "accept" if overall_score >= threshold else "refine"
        
        # Extract feedback
        feedback_pattern = r'feedback:.*?([^\n]+(?:\n[^\n]+)*)'
        feedback_match = re.search(feedback_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)
        
        feedback = ""
        if feedback_match:
            feedback = feedback_match.group(1).strip()
        else:
            # Use the whole text if no specific feedback section is found
            feedback = evaluation_text
        
        return {
            "scores": scores,
            "overall_score": overall_score,
            "suggestions": suggestions,
            "action_needed": action_needed,
            "feedback": feedback
        }
    
    async def _generate_simple_evaluation(self, content: str, original_request: str, content_type: str = "general") -> Dict:
        """
        Generate a simple evaluation when the structured parsing fails.
        
        Args:
            content: The content to evaluate
            original_request: The original request
            content_type: The type of content being evaluated
            
        Returns:
            A simplified evaluation dictionary
        """
        simple_prompt = f"""
        Evaluate this {content_type} on a scale of 0.0 to 1.0:
        
        ORIGINAL REQUEST: {original_request}
        
        CONTENT TO EVALUATE:
        {content}
        
        Provide your evaluation in this exact format:
        
        Score: [a single number between 0.0 and 1.0]
        Feedback: [your feedback]
        Action: [accept/refine/reject]
        """
        
        simple_evaluation = await self.generate_response(
            prompt=simple_prompt,
            system_message=CRITIC_SYSTEM_MESSAGE
        )
        
        # Extract score
        import re
        score_match = re.search(r'Score:\s*(\d+\.\d+|\d+)', simple_evaluation)
        score = float(score_match.group(1)) if score_match else 0.5
        
        # Extract feedback
        feedback_match = re.search(r'Feedback:\s*(.*?)(?:\n|$)', simple_evaluation, re.DOTALL)
        feedback = feedback_match.group(1).strip() if feedback_match else simple_evaluation
        
        # Extract action
        action_match = re.search(r'Action:\s*(accept|refine|reject)', simple_evaluation, re.IGNORECASE)
        action = action_match.group(1).lower() if action_match else ("accept" if score >= 0.7 else "refine")
        
        return {
            "scores": {"overall": score},
            "overall_score": score,
            "suggestions": [],
            "action_needed": action,
            "feedback": feedback,
            "note": "Simple evaluation fallback used"
        }
        
    async def generate_improvement_suggestions(self, content: str, evaluation: Dict) -> List[str]:
        """
        Generate specific improvement suggestions based on evaluation.
        
        Args:
            content: The content that was evaluated
            evaluation: The evaluation results
            
        Returns:
            List of specific improvement suggestions
        """
        if "suggestions" in evaluation and evaluation["suggestions"]:
            return evaluation["suggestions"]
        
        # If no suggestions in the evaluation, generate them
        suggestion_prompt = f"""
        Based on this evaluation:
        
        {json.dumps(evaluation, indent=2)}
        
        Please provide 3-5 specific, actionable suggestions to improve this content:
        
        {content}
        
        Format each suggestion as a single sentence starting with an action verb.
        """
        
        suggestions_response = await self.generate_response(
            prompt=suggestion_prompt,
            system_message=CRITIC_SYSTEM_MESSAGE
        )
        
        # Extract suggestions (one per line)
        import re
        suggestion_lines = re.findall(r'^\d+\.\s*(.*?)$', suggestions_response, re.MULTILINE)
        
        if not suggestion_lines:
            # Try another pattern
            suggestion_lines = re.findall(r'[-•*]\s*(.*?)(?:\n|$)', suggestions_response)
        
        if not suggestion_lines:
            # Split by newlines as a last resort
            suggestion_lines = [line.strip() for line in suggestions_response.split('\n') if line.strip()]
        
        return [line for line in suggestion_lines if line]
